\documentclass[a4paper, 11pt]{article}
 
\usepackage[latin1]{inputenc}    
\usepackage[OT1]{fontenc}
\usepackage[frenchb]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{top=1 cm, bottom=2 cm, left=2.5 cm, right=2.5 cm}

 
\begin{document}
 
\title{Rapport de la Graduate School SGP 2015}
\author{Mathieu Carri\`ere}
\date{}
 
\maketitle

Cette Graduate School a pris place juste avant la conf\'erence SGP (Symposium on Geometry Processing) 
proprement dite. De mani\`ere g\'en\'erale, les cours propos\'es avaient tous un lien plus ou moins
direct avec l'analyse de forme (forme signifiant ici, le plus souvent, vari\'et\'e compacte de dimension 2 
(surfaces 3D), mais pouvant aussi parfois d\'esigner d'autres types d'objets, comme les images 2D par exemple) 
qui est un des domaines math\'ematiques auquel je suis le plus souvent confront\'e dans mon travail de th\`ese.  

\begin{center} \textbf{Cours 1. Optimization (J. Solomon \& D. Bommes)} \end{center}

Le cours est en deux parties. Durant la premi\`ere, le cours couvre plusieurs sujets. En particulier, il commence par traiter les probl\`emes d'optimisation
lin\'eaire (quand la fonction \`a minimiser est lin\'eaire) et leurs applications en param\'etrisation 2D de maillages 3D.
Le plus souvent, l'optimisation se r\'eduit \`a un calcul des moindres carr\'es pond\'er\'e par des poids issus du maillage. Le calcul
du Laplacien d'un maillage ainsi que de ses valeurs propres peut aussi se r\'ealiser par ce biais l\`a. La suite de cette premi\`ere
partie concerne l'optimisation de fonctions sans contraintes. Elle pr\'esente donc les m\'ethodes standardes que l'on voit souvent 
en cours de Master (algorithme de Newton et descente de gradient). Enfin elle aborde l'optimisation sous contraintes d'\'egalit\'e,
avec les multiplicateurs de Lagrange. Durant la deuxi\`eme partie, le cours s'\'etend sur les probl\`emes convexes. Sont pr\'esent\'es
les probl\`emes de programmation lin\'eaire, quadratique et semi-d\'efinie et leurs conditions d'optimalit\'e. Finalement, la description de
quelques algorithmes g\'en\'eraux, comme celui du point int\'erieur, a conclu la pr\'esentation. Comme les Machines \`a Support de Vecteurs
ainsi que les Functional Maps utilisent aussi cet outillage math\'ematique, j'\'etais d\'ej\`a familier de plusieurs notions, que j'ai
du beaucoup manipuler pour l'\'ecriture de l'article qui a justement \'et\'e pr\'esent\'e \`a cette conf\'erence. 

\begin{center} \textbf{Cours 2. Variational Time Integrators (A. Sageman-Furnas)} \end{center}

Ce cours est une introduction aux sch\'emas d'int\'egration utilis\'es en simulation physique. Cela peut avoir beaucoup d'int\'er\^et,
notamment dans les calculs de rendus en images de synth\`ese (Computer Graphics). Toute la pr\'esentation est focalis\'ee autour
de la reconstruction physique de syst\`emes et trajectoires simples, type pendule. L'id\'ee est de g\'en\'eraliser les lois de
Newton classiques avec le principe de moindre action, qui stipule que la seule trajectoire possible est celle qui minimise l'action
(c'est-\`a-dire l'int\'egrale) d'une quantit\'e appel\'ee le Lagrangien. Le calcul variationnel conduit alors aux \'equations d'Euler-Lagrange.
La principale force de ces \'equations r\'eside dans le fait qu'elles conduisent \`a des sch\'emas num\'eriques simplectiques, ce qui veut dire que 
les variations au cours du temps de la diff\'erence entre l'\'energie calcul\'ee par le sch\'ema et la v\'eritable \'energie du cas continu restent born\'ees
(contrairement aux autres sch\'emas).

\begin{center} \textbf{Cours 3. Mappings (N. Aigerman, S. Kovalsky \& R. Poranne)} \end{center}

Ce cours traite des fonctions que l'on peut d\'efinir sur des maillages, que ce soit pour param\'etriser ceux-ci (lorsque les fonctions sont \`a valeurs dans
$\mathbb{R}^n$) ou bien pour trouver des correspondances entre formes diff\'erentes (lorsque les fonctions sont \`a valeurs dans un autre maillage).
Il existe plusieurs mani\`eres de caract\'eriser la qualit\'e d'une fonction. En particulier, les notions de bijectivit\'e locale et de distortion sont
introduites. Le Jacobien de la fonction permet de les quantifier afin d'assurer de bonnes propri\'et\'es pour le mapping.
Dans le cas particulier des complexes simpliciaux (qui sont souvent les approximations consid\'er\'ees en pratique des formes continues),
plusieurs m\'ethodes d'optimisation de la fonction existent. Elles se basent toutes sur la d\'ecomposition du mapping sur une base,
qui permet d'optimiser ais\'ement la fonction avec plusieurs \'energies diff\'erentes. Ce cours (et les mappings de formes en g\'en\'eral) m'a \'et\'e tr\`es utile,
en grande partie car c'est un sujet qui m'int\'eresse et qui pourrait \^etre en lien avec mes travaux actuels.

\begin{center} \textbf{Cours 4. Spectral Processing (M. Kazhdan)} \end{center}

Ce cours couvre le calcul et les applications en analyse de formes du Laplacien d'un maillage et de son spectre.
En effet, il se trouve que la d\'ecomposition d'un signal (c'est-\`a-dire d'une fonction) sur les vecteurs propres du Laplacien
permet de r\'esoudre de nombreux probl\`emes, comme le lissage et l'aiguisage de couleurs et/ou de formes sur un maillage (filtrage haute/basse fr\'equence), ou
bien la r\'esolution d'\'equations aux d\'eriv\'ees partielles standards (\'equation de la chaleur, \'equation d'ondes) sur les formes.
Une grande partie du cours est d\'evolue au calcul du Laplacien d'un signal dans le cas discret quand celui-ci est d\'ecompos\'e sur la
base de fonctions-chapeaux, puis au calcul de la d\'ecomposition proprement dite. Plusieurs algorithmes sont alors donn\'es pour le calcul effectif, 
qui sont peu co\^uteux en temps et en espace.   

\begin{center} \textbf{Cours 5. Skinning (A. Jacobson)} \end{center}

Ce cours explique les diff\'erentes m\'ethodes utilis\'ees pour mod\'eliser le comportement de la "peau" d'une forme 3D, afin
de pouvoir animer cette forme de mani\`ere r\'ealiste, ce qui constitue un v\'eritable challenge au niveau des articulations de la forme par exemple.
Il y a deux techniques tr\`es classiques. La premi\`ere calcule d'abord un squelette de la forme. Ce squelette peut \^etre d\'eform\'e \`a loisir,
le comportement de la surface \'etant r\'egl\'e sur celui de son squelette. La deuxi\`eme m\'ethode consiste \`a enfermer la forme dans une cage
polygonale (\'evidemment la forme doit \^etre suppos\'ee compacte), et, de m\^eme, c'est la manipulation de cette cage qui guidera l'animation de 
la forme correspondante. La plupart du temps, c'est une combinaison de ces deux solutions qui est utilis\'ee, car chacune des deux m\'ethodes
pr\'esente des d\'efauts mais aussi des avantages dans des situations particuli\`eres. Un probl\`eme commun est n\'eanmoins le design de poids
\`a assigner aux segments (que ce soit pour les os ou les polygones). Pendant longtemps, ce design a \'et\'e calcul\'e \`a la main par des programmateurs,
mais des techniques supervis\'ees de calcul automatique de poids commencent \`a voir le jour.

\begin{center} \textbf{Cours 6. Machine Learning Techniques for Geometric Modeling (E. Kalogerakis)} \end{center}

Ce cours traite de l'apport de l'apprentissage automatique en analyse de formes. C'est un sujet auquel je suis plut\^ot familier,
car il constitue le point central de l'article que nous avons soumis \`a cette conf\'erence.
Le cours est une grosse introduction au Machine Learning, ponctu\'ee d'applications en analyse de formes, en particulier
la synth\`ese et la segmentation automatique de formes. Les mod\`eles les plus standards sont pr\'esent\'es, qu'ils soient
param\'etriques, comme la r\'egression logistique, ou non, comme les techniques d'analyse en composantes principales ou les
algorithmes standards de classification supervis\'ee (Machines \`a Support de Vecteurs, r\'eseaux neuronaux, Boosting).
Beaucoup de temps est accord\'e \`a l'explication des phases d'apprentissage particuli\`eres en g\'eom\'etrie processing, 
en cela qu'elles caract\'erisent les nouvelles applications du Machine Learning en analyse de formes.

\begin{center} \textbf{Cours 7. Registration (S. Bouaziz \& A. Tagliasacchi)} \end{center}

Ce cours final est une introduction au recalage, c'est-\`a-dire aux m\'ethodes permettant d'aligner une forme sur une
deuxi\`eme. Dans tous les cas pr\'esent\'es (images ou formes 3D), le recalage optimal minimise deux \'energies diff\'erentes,
une \'energie associ\'ee au matching entre les formes et une autre associ\'ee \`a la transformation utilis\'ee.
Cette deuxi\`eme \'energie varie beaucoup en fonction des mod\`eles d'entr\'ee (formes rigides, articul\'ees, \'elastiques, fluides...)
et la plupart des cours consiste en une description exhaustive des diff\'erents mod\`eles.

\end{document}
